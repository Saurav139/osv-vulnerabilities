{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-02-25T22:15:25.4680378Z",
              "execution_start_time": "2025-02-25T22:13:52.390311Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "d20d142c-b438-467e-8790-7b95cda3975d",
              "queued_time": "2025-02-25T22:12:25.540291Z",
              "session_id": "7",
              "session_start_time": "2025-02-25T22:12:25.5418042Z",
              "spark_jobs": null,
              "spark_pool": "osv",
              "state": "finished",
              "statement_id": 2,
              "statement_ids": [
                2
              ]
            },
            "text/plain": [
              "StatementMeta(osv, 7, 2, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of duplicate records: 0\n",
            "+---+-------+--------+-----+\n",
            "|id |summary|modified|count|\n",
            "+---+-------+--------+-----+\n",
            "+---+-------+--------+-----+\n",
            "\n",
            "Number of records with future dates: 0\n",
            "+---+-------+-------+-------+--------+---------+-----------------+--------+--------------+--------+---------+----+\n",
            "|id |summary|details|aliases|modified|published|database_specific|affected|schema_version|severity|ecosystem|year|\n",
            "+---+-------+-------+-------+--------+---------+-----------------+--------+--------------+--------+---------+----+\n",
            "+---+-------+-------+-------+--------+---------+-----------------+--------+--------------+--------+---------+----+\n",
            "\n",
            "+-------------+----------+-----------------+--------------+\n",
            "|Total Records|Unique IDs|Unique Ecosystems|Unique Aliases|\n",
            "+-------------+----------+-----------------+--------------+\n",
            "|        42802|     42802|                9|         28015|\n",
            "+-------------+----------+-----------------+--------------+\n",
            "\n",
            "Data processing completed in 88.55 seconds.\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count, when, lit, explode, expr, array_contains\n",
        "from pyspark.sql.utils import AnalysisException\n",
        "import time\n",
        "from pyspark.sql.functions import current_date,countDistinct\n",
        "\n",
        "# Initialize Spark Session\n",
        "#spark = SparkSession.builder.appName(\"OSV Data Processing\").getOrCreate()\n",
        "\n",
        "# Define Delta Table Path\n",
        "delta_path = \"#path to delta table\"\n",
        "\n",
        "try:\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Load Delta Table\n",
        "    df = spark.read.format(\"delta\").load(delta_path)\n",
        "\n",
        "    # Step 1: Data Quality Checks (Check for duplicates,future dates,cardinality check)\n",
        "    df_duplicates = df.groupBy(\"id\", \"summary\", \"modified\").count().filter(col(\"count\") > 1)\n",
        "    print(f\"Number of duplicate records: {df_duplicates.count()}\")\n",
        "    df_duplicates.show(truncate=False)\n",
        "\n",
        "    df_future_dates = df.filter(col(\"published\") > current_date())\n",
        "    print(f\"Number of records with future dates: {df_future_dates.count()}\")\n",
        "    df_future_dates.show(truncate=False)\n",
        "\n",
        "    df.select([\n",
        "    count(\"id\").alias(\"Total Records\"),\n",
        "    countDistinct(\"id\").alias(\"Unique IDs\"),\n",
        "    countDistinct(\"ecosystem\").alias(\"Unique Ecosystems\"),\n",
        "    countDistinct(\"aliases\").alias(\"Unique Aliases\")\n",
        "]).show()\n",
        "\n",
        "    # Step 2: Extract Required Fields for Derived Tables\n",
        "    df_exploded = (\n",
        "        df.withColumn(\"affected\", explode(col(\"affected\")))  # Explode affected array\n",
        "          .withColumn(\"ranges\", explode(col(\"affected.ranges\")))  # Explode ranges\n",
        "          .withColumn(\"events\", explode(col(\"ranges.events\")))  # Explode events\n",
        "          .withColumn(\"introduced\", col(\"events.introduced\"))  # Extract introduced version\n",
        "          .withColumn(\"fixed_version\", col(\"events.fixed\"))  # Extract fixed version\n",
        "          .withColumn(\"ecosystem\", col(\"affected.package.ecosystem\"))  # Extract ecosystem directly\n",
        "          .withColumn(\"package_name\", col(\"affected.package.name\"))  # Extract package name\n",
        "          .select(\"id\", \"ecosystem\", \"package_name\", \"introduced\", \"fixed_version\", \"published\", \"severity\")\n",
        "    )\n",
        "\n",
        "    # Step 3: Create Derived Tables for Common Query Patterns\n",
        "\n",
        "    # Derived Table 1: Get vulnerabilities by package\n",
        "    vulnerabilities_by_package = df_exploded.select(\"id\", \"ecosystem\", \"package_name\", \"severity\", \"introduced\", \"fixed_version\")\n",
        "    vulnerabilities_by_package.write.format(\"delta\").partitionBy(\"ecosystem\").mode(\"overwrite\").save(delta_path + \"derived/vulnerabilities_by_package\")\n",
        "\n",
        "    # Derived Table 2: Get vulnerabilities by ecosystem\n",
        "    vulnerabilities_by_ecosystem = df_exploded.groupBy(\"ecosystem\").count()\n",
        "    vulnerabilities_by_ecosystem.write.format(\"delta\").mode(\"overwrite\").save(delta_path + \"derived/vulnerabilities_by_ecosystem\")\n",
        "\n",
        "    # Derived Table 3: Get fixed versions of vulnerabilities\n",
        "    fixed_versions = df_exploded.filter(col(\"fixed_version\").isNotNull())\n",
        "    fixed_versions.write.format(\"delta\").partitionBy(\"ecosystem\").mode(\"overwrite\").save(delta_path + \"derived/fixed_versions\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    \n",
        "    print(f\"Data processing completed in {round(end_time - start_time, 2)} seconds.\")\n",
        "\n",
        "except AnalysisException as e:\n",
        "    print(f\"Spark AnalysisException: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"Unexpected Error: {e}\")\n",
        "\n",
        "finally:\n",
        "    # Stop Spark Session\n",
        "    spark.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
