{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Part 2 - Data Lake Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "- ## Use Schema fromJSON\n",
        "- ## Read parquet files\n",
        "- ## Print schema and dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-02-25T20:55:05.9132401Z",
              "execution_start_time": null,
              "livy_statement_state": null,
              "normalized_state": "finished",
              "parent_msg_id": "03858f00-de13-4388-8645-df2d2a1251ca",
              "queued_time": "2025-02-25T20:55:05.8645153Z",
              "session_id": null,
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": null,
              "state": "finished",
              "statement_id": -1,
              "statement_ids": []
            },
            "text/plain": [
              "StatementMeta(, , -1, Finished, , Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "LIVY_JOB_TIMED_OUT",
          "evalue": "Livy session has failed. Session state: Dead. Error code: LIVY_JOB_TIMED_OUT. Job failed during run time with state=[dead]. Source: Unknown.",
          "output_type": "error",
          "traceback": [
            "LIVY_JOB_TIMED_OUT: Livy session has failed. Session state: Dead. Error code: LIVY_JOB_TIMED_OUT. Job failed during run time with state=[dead]. Source: Unknown."
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, BooleanType\n",
        "\n",
        "# Start Spark SessionL\n",
        "spark = SparkSession.builder.appName(\"OSV Data Lake \").getOrCreate()\n",
        "\n",
        "# Define Correct Schema\n",
        "schema = StructType([\n",
        "    StructField(\"id\", StringType(), True),\n",
        "    StructField(\"summary\", StringType(), True),\n",
        "    StructField(\"details\", StringType(), True),\n",
        "    StructField(\"aliases\", ArrayType(StringType()), True),\n",
        "    StructField(\"modified\", StringType(), True),\n",
        "    StructField(\"published\", StringType(), True),\n",
        "    StructField(\"database_specific\", StructType([\n",
        "        StructField(\"nvd_published_at\", StringType(), True),  # Force as String\n",
        "        StructField(\"severity\", StringType(), True),\n",
        "        StructField(\"github_reviewed\", BooleanType(), True),\n",
        "        StructField(\"github_reviewed_at\", StringType(), True)\n",
        "    ]), True),\n",
        "    StructField(\"affected\", ArrayType(StructType([\n",
        "        StructField(\"package\", StructType([\n",
        "            StructField(\"name\", StringType(), True),\n",
        "            StructField(\"ecosystem\", StringType(), True),\n",
        "            StructField(\"purl\", StringType(), True)\n",
        "        ]), True),\n",
        "        StructField(\"ranges\", ArrayType(StructType([\n",
        "            StructField(\"type\", StringType(), True),\n",
        "            StructField(\"events\", ArrayType(StructType([\n",
        "                StructField(\"introduced\", StringType(), True),\n",
        "                StructField(\"fixed\", StringType(), True)\n",
        "            ]), True))\n",
        "        ]), True), True)\n",
        "    ]), True), True),\n",
        "    StructField(\"schema_version\", StringType(), True),\n",
        "    StructField(\"severity\", ArrayType(StructType([\n",
        "        StructField(\"type\", StringType(), True),\n",
        "        StructField(\"score\", StringType(), True)\n",
        "    ])), True)\n",
        "])\n",
        "\n",
        "# Load Parquet with Correct Schema\n",
        "df = spark.read.schema(schema).parquet(\"# Enter Path to Parquet Files\")\n",
        "\n",
        "# Show Schema & Data\n",
        "df.printSchema()\n",
        "df.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Store parquet files in a delta table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from delta import DeltaTable\n",
        "\n",
        "delta_path = \"# Enter Delta Path Here\"\n",
        "\n",
        "# Convert Parquet to Delta\n",
        "df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Read the delta table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_delta = spark.read.format(\"delta\").load(delta_path)\n",
        "df_delta.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Generate ecosystem and year columns from the data avaialble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-02-25T19:31:11.3394588Z",
              "execution_start_time": "2025-02-25T19:31:11.1858805Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "4c96309f-85de-42a0-857a-db1a5c623b94",
              "queued_time": "2025-02-25T19:31:11.0887935Z",
              "session_id": "0",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "osv",
              "state": "finished",
              "statement_id": 30,
              "statement_ids": [
                30
              ]
            },
            "text/plain": [
              "StatementMeta(osv, 0, 30, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, expr\n",
        "\n",
        "# Extract the first ecosystem from affected.package.ecosystem\n",
        "df = df.withColumn(\"ecosystem\", expr(\"affected[0].package.ecosystem\"))\n",
        "df = df.withColumn(\"year\", col(\"published\").substr(1, 4))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Partitioning by ecosystem and year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-02-25T19:39:08.9351019Z",
              "execution_start_time": "2025-02-25T19:31:21.3255317Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "11dc012b-cc12-4d4a-ba9a-6da91f3adf44",
              "queued_time": "2025-02-25T19:31:21.2388787Z",
              "session_id": "0",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "osv",
              "state": "finished",
              "statement_id": 31,
              "statement_ids": [
                31
              ]
            },
            "text/plain": [
              "StatementMeta(osv, 0, 31, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df.write.format(\"delta\") \\\n",
        "    .partitionBy(\"ecosystem\", \"year\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"overwriteSchema\", \"true\") \\\n",
        "    .save(delta_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Time travel to see previous version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-02-25T19:39:41.3143341Z",
              "execution_start_time": "2025-02-25T19:39:34.1973302Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "b30bb6a9-ede5-44dd-97c1-bdcf06210865",
              "queued_time": "2025-02-25T19:39:34.1109954Z",
              "session_id": "0",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "osv",
              "state": "finished",
              "statement_id": 33,
              "statement_ids": [
                33
              ]
            },
            "text/plain": [
              "StatementMeta(osv, 0, 33, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+-------+--------------------+--------------------+--------------------+--------------------+-----------------+--------------------+--------------+--------------------+---------+----+\n",
            "|            id|summary|             details|             aliases|            modified|           published|database_specific|            affected|schema_version|            severity|ecosystem|year|\n",
            "+--------------+-------+--------------------+--------------------+--------------------+--------------------+-----------------+--------------------+--------------+--------------------+---------+----+\n",
            "| PYSEC-2017-84|   null|An issue was disc...|[CVE-2017-16613, ...|2024-05-01T11:41:...|2017-11-21T13:29:00Z|             null|[{{swauth, PyPI, ...|         1.6.0|                null|     PyPI|2017|\n",
            "| PYSEC-2017-36|   null|Directory travers...|[CVE-2017-14695, ...|2024-04-22T23:26:...|2017-10-24T17:29:00Z|             null|[{{salt, PyPI, pk...|         1.6.0|                null|     PyPI|2017|\n",
            "|PYSEC-2017-113|   null|Integer overflow ...|[CVE-2017-6952, G...|2024-11-21T22:42:...|2017-03-16T21:59:00Z|             null|[{{capstone, PyPI...|         1.6.0|[{CVSS_V3, CVSS:3...|     PyPI|2017|\n",
            "|PYSEC-2017-146|   null|Apache Ignite 1.0...|                null|2024-11-21T14:22:...|2017-06-28T13:29:00Z|             null|[{{pyignite, PyPI...|         1.6.0|[{CVSS_V3, CVSS:3...|     PyPI|2017|\n",
            "| PYSEC-2017-22|   null|An exploitable vu...|[CVE-2017-16618, ...|2023-11-08T03:59:...|2017-11-08T03:29:00Z|             null|[{{owlmixin, PyPI...|         1.6.0|                null|     PyPI|2017|\n",
            "| PYSEC-2017-78|   null|An exploitable vu...|[CVE-2017-16763, ...|2023-11-08T03:59:...|2017-11-10T09:29:00Z|             null|[{{confire, PyPI,...|         1.6.0|                null|     PyPI|2017|\n",
            "| PYSEC-2017-90|   null|In Mercurial befo...|[CVE-2017-17458, ...|2024-05-01T17:28:...|2017-12-07T18:29:00Z|             null|[{{mercurial, PyP...|         1.6.0|                null|     PyPI|2017|\n",
            "| PYSEC-2017-24|   null|In PyJWT 1.5.0 an...|[CVE-2017-11424, ...|2023-11-08T03:58:...|2017-08-24T16:29:00Z|             null|[{{pyjwt, PyPI, p...|         1.6.0|                null|     PyPI|2017|\n",
            "|PYSEC-2017-103|   null|An incorrect impl...|[CVE-2017-5591, G...|2023-11-08T03:59:...|2017-02-09T20:59:00Z|             null|[{{sleekxmpp, PyP...|         1.6.0|                null|     PyPI|2017|\n",
            "| PYSEC-2017-52|   null|Plone 3.3.0 throu...|[CVE-2015-7315, G...|2023-11-08T03:57:...|2017-09-25T17:29:00Z|             null|[{{plone, PyPI, p...|         1.6.0|                null|     PyPI|2017|\n",
            "| PYSEC-2017-93|   null|A HTTP/2 implemen...|[CVE-2016-6580, G...|2024-08-30T23:58:...|2017-01-10T15:59:00Z|             null|[{{priority, PyPI...|         1.6.0|                null|     PyPI|2017|\n",
            "| PYSEC-2017-68|   null|The Recurly Clien...|[CVE-2017-0906, G...|2023-11-08T03:58:...|2017-11-13T17:29:00Z|             null|[{{recurly, PyPI,...|         1.6.0|                null|     PyPI|2017|\n",
            "|  PYSEC-2017-5|   null|An exploitable vu...|[CVE-2017-2809, G...|2023-11-08T03:59:...|2017-09-14T19:29:00Z|             null|[{{ansible-vault,...|         1.6.0|                null|     PyPI|2017|\n",
            "|PYSEC-2017-145|   null|OpenStack Compute...|[CVE-2015-2687, G...|2024-11-25T22:42:...|2017-08-09T18:29:00Z|             null|[{{nova, PyPI, pk...|         1.6.0|[{CVSS_V3, CVSS:3...|     PyPI|2017|\n",
            "| PYSEC-2017-97|   null|file_open in Tryt...|[CVE-2017-0360, G...|2024-04-22T23:11:...|2017-04-04T17:59:00Z|             null|[{{trytond, PyPI,...|         1.6.0|                null|     PyPI|2017|\n",
            "|  PYSEC-2017-6|   null|attic before 0.15...|[CVE-2015-4082, G...|2024-05-01T11:26:...|2017-08-18T16:29:00Z|             null|[{{attic, PyPI, p...|         1.6.0|                null|     PyPI|2017|\n",
            "| PYSEC-2017-65|   null|protobuf allows r...|[CVE-2015-5237, G...|2023-11-08T03:57:...|2017-09-25T17:29:00Z|             null|[{{protobuf, PyPI...|         1.6.0|                null|     PyPI|2017|\n",
            "| PYSEC-2017-43|   null|Cross-site script...|[CVE-2016-10516, ...|2023-11-08T03:58:...|2017-10-23T16:29:00Z|             null|[{{werkzeug, PyPI...|         1.6.0|                null|     PyPI|2017|\n",
            "| PYSEC-2017-38|   null|When using the lo...|[CVE-2017-5192, G...|2024-04-22T22:41:...|2017-09-26T14:29:00Z|             null|[{{salt, PyPI, pk...|         1.6.0|                null|     PyPI|2017|\n",
            "|PYSEC-2017-125|   null|There is a Mismat...|                null|2024-11-21T14:22:...|2017-07-24T01:29:00Z|             null|[{{exiv2, PyPI, p...|         1.6.0|[{CVSS_V3, CVSS:3...|     PyPI|2017|\n",
            "+--------------+-------+--------------------+--------------------+--------------------+--------------------+-----------------+--------------------+--------------+--------------------+---------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_time_travel = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(delta_path)\n",
        "df_time_travel.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Using sort within paritions as an indexing strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-02-25T19:51:45.6463985Z",
              "execution_start_time": "2025-02-25T19:44:56.3547722Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "8872ce4d-2096-4ceb-8150-8bda20e83e39",
              "queued_time": "2025-02-25T19:44:56.263787Z",
              "session_id": "0",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "osv",
              "state": "finished",
              "statement_id": 38,
              "statement_ids": [
                38
              ]
            },
            "text/plain": [
              "StatementMeta(osv, 0, 38, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Repartition by ecosystem (to align partitions properly)\n",
        "df = df.repartition(\"ecosystem\")\n",
        "\n",
        "# Sort data within each partition manually\n",
        "df = df.sortWithinPartitions(\"ecosystem\", \"year\")\n",
        "\n",
        "# Write sorted data into Delta format\n",
        "df.write.format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"ecosystem\", \"year\") \\\n",
        "    .save(delta_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Enablng vaccum and retentiion policies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-02-25T20:52:39.8131024Z",
              "execution_start_time": null,
              "livy_statement_state": null,
              "normalized_state": "finished",
              "parent_msg_id": "03d47cad-8e19-4787-8769-72e745bcbf54",
              "queued_time": "2025-02-25T20:52:39.7574261Z",
              "session_id": null,
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": null,
              "state": "finished",
              "statement_id": -1,
              "statement_ids": []
            },
            "text/plain": [
              "StatementMeta(, , -1, Finished, , Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "LIVY_JOB_TIMED_OUT",
          "evalue": "Livy session has failed. Session state: Dead. Error code: LIVY_JOB_TIMED_OUT. Job failed during run time with state=[dead]. Source: Unknown.",
          "output_type": "error",
          "traceback": [
            "LIVY_JOB_TIMED_OUT: Livy session has failed. Session state: Dead. Error code: LIVY_JOB_TIMED_OUT. Job failed during run time with state=[dead]. Source: Unknown."
          ]
        }
      ],
      "source": [
        "deltaTable.vacuum(75)  # Keep only the last 75 days of data\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
